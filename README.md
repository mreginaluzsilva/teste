# ğŸ“ Pipeline ETL de Alunos

Projeto de Engenharia de Dados para processamento, validaÃ§Ã£o e armazenamento de dados acadÃªmicos.

## ğŸ“‹ Sobre o Projeto
Este projeto simula um pipeline **ETL (Extract, Transform, Load)** completo. O sistema processa dados brutos de alunos (gerados via script), aplica regras de negÃ³cio para validar consistÃªncia (notas, frequÃªncia, idade), separa registros invÃ¡lidos para auditoria e armazena os dados confiÃ¡veis em um Banco de Dados SQL.

**Objetivo:** Demonstrar um fluxo profissional de tratamento de dados com rastreabilidade (Logs) e persistÃªncia em banco relacional.

## ğŸš€ Tecnologias Utilizadas
* **Python 3.12+**
* **Pandas:** ManipulaÃ§Ã£o e transformaÃ§Ã£o de dados.
* **SQLite:** Banco de dados relacional (Serverless).
* **Faker:** GeraÃ§Ã£o de dados sintÃ©ticos realistas.
* **Logging:** Monitoramento e registro de execuÃ§Ã£o.
* **Git/GitHub:** Versionamento de cÃ³digo.

## âš™ï¸ Arquitetura do Pipeline

1.  **Extract:** Leitura de arquivos CSV brutos (`data/raw`).
2.  **Transform:**
    * Limpeza de dados (tratamento de nulos).
    * ValidaÃ§Ã£o de Regras de NegÃ³cio (Nota entre 0-10, FrequÃªncia positiva).
    * SeparaÃ§Ã£o: Dados VÃ¡lidos vs. Dados com Erro.
3.  **Load:**
    * âœ… **Dados VÃ¡lidos:** Salvos na tabela `alunos_tartados` (SQLite).
    * âš ï¸ **Dados InvÃ¡lidos:** Salvos em `relatorio_erros.csv` para auditoria.

## ğŸ“‚ Estrutura de Arquivos

projeto-etl/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/             # Onde fica o CSV bruto
â”‚   â””â”€â”€ processed/       # Onde o Banco de Dados e RelatÃ³rios sÃ£o salvos
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract.py       # MÃ³dulo de leitura
â”‚   â”œâ”€â”€ transform.py     # LÃ³gica de validaÃ§Ã£o e limpeza
â”‚   â”œâ”€â”€ load.py          # MÃ³dulo de salvamento (CSV e SQL)
â”‚   â”œâ”€â”€ main.py          # Orquestrador do pipeline (com Logs)
â”‚   â”œâ”€â”€ gerar_fakes.py   # Gerador de massa de dados
â”‚   â””â”€â”€ consultar_dados.py # Script para testar o banco SQL
â”‚
â”œâ”€â”€ pipeline.log         # DiÃ¡rio de execuÃ§Ã£o do sistema
â”œâ”€â”€ requirements.txt     # Lista de dependÃªncias
â””â”€â”€ README.md            # DocumentaÃ§Ã£o


## ğŸ› ï¸ Como Executar

### 1. Preparar o ambiente
Certifique-se de ter o Python instalado. Recomenda-se usar um ambiente virtual:

```bash
# Windows
python -m venv .venv
.\.venv\Scripts\Activate

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 2. Gerar dados de teste
Execute o script para criar 1.000 alunos com dados fictÃ­cios (nomes, notas e falhas propositais):
```bash
python scripts/gerar_fakes.py
```

### 3. Rodar o Pipeline ETL
Processe os dados. O terminal mostrarÃ¡ o progresso via Logs.
```bash
python scripts/main.py
```

### 4. Verificar os resultados
* Abra o arquivo `pipeline.log` para ver os detalhes da execuÃ§Ã£o.
* Verifique a pasta `data/processed` para encontrar o `relatorio_erros.csv`.
* Para consultar o Banco de Dados, rode:
```bash
python scripts/consultar_dados.py
```

---
Desenvolvido para fins de estudo em Engenharia de Dados.